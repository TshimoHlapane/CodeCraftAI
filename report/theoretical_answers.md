## Q1: Explain how AI-driven code generation tools (e.g., GitHub Copilot) reduce development time. What are their limitations?
(Write answer here)

## Q2: Compare supervised and unsupervised learning in the context of automated bug detection.
(Write answer here)

## Q3: Why is bias mitigation critical when using AI for user experience personalization?
**Bias mitigation is critical when using AI for user experience personalization** because it ensures that the system treats all users fairly and inclusively. Personalization algorithms often rely on historical user data, which can reflect existing social, cultural, or demographic biases. If not addressed, these biases can lead to unequal treatment—such as showing limited opportunities, reinforcing stereotypes, or excluding certain user groups entirely.

For example, an AI system that prioritizes content or recommendations based on biased training data may consistently favor one gender, age group, or region over others. This not only undermines user trust but also reduces the quality and fairness of the user experience.

By implementing bias mitigation techniques—such as using fairness-aware algorithms, diversifying training data, or regularly auditing outputs—developers can help ensure that AI-driven personalization serves all users equitably and respects ethical standards. This leads to more inclusive software, protects brand reputation, and aligns with responsible AI practices.
