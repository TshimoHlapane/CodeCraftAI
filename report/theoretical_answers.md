## Q1: Explain how AI-driven code generation tools (e.g., GitHub Copilot) reduce development time. What are their limitations?
AI-driven code generation tools like GitHub Copilot significantly reduce development time by providing intelligent code suggestions, auto-completing functions, and generating boilerplate code. These tools leverage large language models trained on vast codebases, enabling them to predict and suggest contextually relevant code snippets as developers type. This accelerates routine tasks, reduces the need to search for syntax or documentation, and helps maintain coding momentum. For example, Copilot can quickly generate repetitive functions, suggest test cases, or even write documentation, allowing developers to focus on higher-level problem-solving.

However, these tools have limitations. They may produce code that is syntactically correct but semantically incorrect or insecure, especially for complex or domain-specific tasks. Over-reliance on AI suggestions can lead to reduced code understanding and critical thinking. Additionally, AI-generated code may inadvertently introduce bugs, security vulnerabilities, or licensing issues if not carefully reviewed. Therefore, while AI code generation tools are valuable productivity aids, developers must validate and understand the code they accept.

## Q2: Compare supervised and unsupervised learning in the context of automated bug detection.
(Write answer here)

## Q3: Why is bias mitigation critical when using AI for user experience personalization?
**Bias mitigation is critical when using AI for user experience personalization** because it ensures that the system treats all users fairly and inclusively. Personalization algorithms often rely on historical user data, which can reflect existing social, cultural, or demographic biases. If not addressed, these biases can lead to unequal treatment—such as showing limited opportunities, reinforcing stereotypes, or excluding certain user groups entirely.

For example, an AI system that prioritizes content or recommendations based on biased training data may consistently favor one gender, age group, or region over others. This not only undermines user trust but also reduces the quality and fairness of the user experience.

By implementing bias mitigation techniques—such as using fairness-aware algorithms, diversifying training data, or regularly auditing outputs—developers can help ensure that AI-driven personalization serves all users equitably and respects ethical standards. This leads to more inclusive software, protects brand reputation, and aligns with responsible AI practices.
